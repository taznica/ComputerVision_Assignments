{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_2_data_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyParGCjljyxCV2tKlzH57k7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taznica/ComputerVision_Assignments/blob/main/assignment_2_data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5GULPoNOeJW"
      },
      "source": [
        "#####################################################\n",
        "### --- We modify the dataloader in this block ---###\n",
        "#####################################################\n",
        "\n",
        "from __future__ import print_function\n",
        "import torchvision.datasets.vision as vision\n",
        "import warnings\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import codecs\n",
        "# from torchvision.datasets.utils import download_url, download_and_extract_archive, extract_archive, \\\n",
        "#    makedir_exist_ok, verify_str_arg\n",
        "from torchvision.datasets.utils import download_url, download_and_extract_archive, extract_archive, \\\n",
        "    verify_str_arg\n",
        "\n",
        "# This class is copied from torchvision.datasets.MNIST\n",
        "class MyMNIST(vision.VisionDataset):\n",
        "\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None,\n",
        "                 download=False, limit_data=None):\n",
        "        super(MyMNIST, self).__init__(root, transform=transform,\n",
        "                                    target_transform=target_transform)\n",
        "        self.train = train  # training set or test set\n",
        "\n",
        "        # ------- We can ingore this block -------\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError('Dataset not found.' +\n",
        "                               ' You can use download=True to download it')\n",
        "        if self.train:\n",
        "            data_file = self.training_file\n",
        "        else:\n",
        "            data_file = self.test_file\n",
        "        # -----------------------------------------------\n",
        "\n",
        "        # We change the lines below; these specify how the data are loaded.\n",
        "        # self.data contain images (type 'torch.Tensor' of [num_images, H, W]) \n",
        "        # self.targets contain labels (class ids) (type 'torch.Tensor' of [num_images]) \n",
        "        # images and labels are stored in the same \n",
        "        self.data, self.targets = torch.load(os.path.join(self.processed_folder, data_file))\n",
        "\n",
        "        # We use only the images and lables whose indeces are in the range of 0..limit_data-1.\n",
        "        if not limit_data is None:\n",
        "          self.data    = self.data[   :limit_data, :,:]\n",
        "          self.targets = self.targets[:limit_data]\n",
        "          if self.train:\n",
        "            print(\"[WRN]: Trainig Data is limited, only the first \"+str(self.data.size(0))+\" samples will be used.\")\n",
        "          else:\n",
        "            print(\"[WRN]: Test Data is limited, only the first \"   +str(self.data.size(0))+\" samples will be used.\")            \n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # We extract the image and label of the specified 'index'.\n",
        "        img, target = self.data[index], int(self.targets[index])\n",
        "\n",
        "        # Prepare for self.transform below.\n",
        "        img = Image.fromarray(img.numpy(), mode='L')\n",
        "\n",
        "        # Transform img.\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# ---------------------- We just leave the code from here as original -----------------\n",
        "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where ``MNIST/processed/training.pt``\n",
        "            and  ``MNIST/processed/test.pt`` exist.\n",
        "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
        "            otherwise from ``test.pt``.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "    \"\"\"\n",
        "\n",
        "    urls = [\n",
        "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
        "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
        "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
        "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
        "    ]\n",
        "    training_file = 'training.pt'\n",
        "    test_file = 'test.pt'\n",
        "    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',\n",
        "               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
        "\n",
        "    @property\n",
        "    def train_labels(self):\n",
        "        warnings.warn(\"train_labels has been renamed targets\")\n",
        "        return self.targets\n",
        "\n",
        "    @property\n",
        "    def test_labels(self):\n",
        "        warnings.warn(\"test_labels has been renamed targets\")\n",
        "        return self.targets\n",
        "\n",
        "    @property\n",
        "    def train_data(self):\n",
        "        warnings.warn(\"train_data has been renamed data\")\n",
        "        return self.data\n",
        "\n",
        "    @property\n",
        "    def test_data(self):\n",
        "        warnings.warn(\"test_data has been renamed data\")\n",
        "        return self.data\n",
        "\n",
        "    @property\n",
        "    def raw_folder(self):\n",
        "        return os.path.join(self.root, self.__class__.__name__, 'raw')\n",
        "\n",
        "    @property\n",
        "    def processed_folder(self):\n",
        "        return os.path.join(self.root, self.__class__.__name__, 'processed')\n",
        "\n",
        "    @property\n",
        "    def class_to_idx(self):\n",
        "        return {_class: i for i, _class in enumerate(self.classes)}\n",
        "\n",
        "    def _check_exists(self):\n",
        "        return (os.path.exists(os.path.join(self.processed_folder,\n",
        "                                            self.training_file)) and\n",
        "                os.path.exists(os.path.join(self.processed_folder,\n",
        "                                            self.test_file)))\n",
        "        \n",
        "    def download(self):\n",
        "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
        "\n",
        "        if self._check_exists():\n",
        "            return\n",
        "\n",
        "        # makedir_exist_ok(self.raw_folder)\n",
        "        # makedir_exist_ok(self.processed_folder)\n",
        "        os.makedirs(self.raw_folder, exist_ok=True)\n",
        "        os.makedirs(self.processed_folder, exist_ok=True)\n",
        "\n",
        "        # download files\n",
        "        for url in self.urls:\n",
        "            filename = url.rpartition('/')[2]\n",
        "            download_and_extract_archive(url, download_root=self.raw_folder, filename=filename)\n",
        "\n",
        "        # process and save as torch files\n",
        "        print('Processing...')\n",
        "\n",
        "        training_set = (\n",
        "            read_image_file(os.path.join(self.raw_folder, 'train-images-idx3-ubyte')),\n",
        "            read_label_file(os.path.join(self.raw_folder, 'train-labels-idx1-ubyte'))\n",
        "        )\n",
        "        test_set = (\n",
        "            read_image_file(os.path.join(self.raw_folder, 't10k-images-idx3-ubyte')),\n",
        "            read_label_file(os.path.join(self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
        "        )\n",
        "        with open(os.path.join(self.processed_folder, self.training_file), 'wb') as f:\n",
        "            torch.save(training_set, f)\n",
        "        with open(os.path.join(self.processed_folder, self.test_file), 'wb') as f:\n",
        "            torch.save(test_set, f)\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return \"Split: {}\".format(\"Train\" if self.train is True else \"Test\")\n",
        "\n",
        "def read_label_file(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        x = read_sn3_pascalvincent_tensor(f, strict=False)\n",
        "    assert(x.dtype == torch.uint8)\n",
        "    assert(x.ndimension() == 1)\n",
        "    return x.long()\n",
        "\n",
        "\n",
        "def read_image_file(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        x = read_sn3_pascalvincent_tensor(f, strict=False)\n",
        "    assert(x.dtype == torch.uint8)\n",
        "    assert(x.ndimension() == 3)\n",
        "    return x\n",
        "\n",
        "def get_int(b):\n",
        "    return int(codecs.encode(b, 'hex'), 16)\n",
        "\n",
        "\n",
        "def open_maybe_compressed_file(path):\n",
        "    \"\"\"Return a file object that possibly decompresses 'path' on the fly.\n",
        "       Decompression occurs when argument `path` is a string and ends with '.gz' or '.xz'.\n",
        "    \"\"\"\n",
        "    if not isinstance(path, torch._six.string_classes):\n",
        "        return path\n",
        "    if path.endswith('.gz'):\n",
        "        import gzip\n",
        "        return gzip.open(path, 'rb')\n",
        "    if path.endswith('.xz'):\n",
        "        import lzma\n",
        "        return lzma.open(path, 'rb')\n",
        "    return open(path, 'rb')\n",
        "\n",
        "\n",
        "def read_sn3_pascalvincent_tensor(path, strict=True):\n",
        "    \"\"\"Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx-io.lsh').\n",
        "       Argument may be a filename, compressed filename, or file object.\n",
        "    \"\"\"\n",
        "    # typemap\n",
        "    if not hasattr(read_sn3_pascalvincent_tensor, 'typemap'):\n",
        "        read_sn3_pascalvincent_tensor.typemap = {\n",
        "            8: (torch.uint8, np.uint8, np.uint8),\n",
        "            9: (torch.int8, np.int8, np.int8),\n",
        "            11: (torch.int16, np.dtype('>i2'), 'i2'),\n",
        "            12: (torch.int32, np.dtype('>i4'), 'i4'),\n",
        "            13: (torch.float32, np.dtype('>f4'), 'f4'),\n",
        "            14: (torch.float64, np.dtype('>f8'), 'f8')}\n",
        "    # read\n",
        "    with open_maybe_compressed_file(path) as f:\n",
        "        data = f.read()\n",
        "    # parse\n",
        "    magic = get_int(data[0:4])\n",
        "    nd = magic % 256\n",
        "    ty = magic // 256\n",
        "    assert nd >= 1 and nd <= 3\n",
        "    assert ty >= 8 and ty <= 14\n",
        "    m = read_sn3_pascalvincent_tensor.typemap[ty]\n",
        "    s = [get_int(data[4 * (i + 1): 4 * (i + 2)]) for i in range(nd)]\n",
        "    parsed = np.frombuffer(data, dtype=m[1], offset=(4 * (nd + 1)))\n",
        "    assert parsed.shape[0] == np.prod(s) or not strict\n",
        "    return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)"
      ],
      "execution_count": 512,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLEQOdWtRod5"
      },
      "source": [
        "#####################################################\n",
        "### --- Define a simple network in this block --- ###\n",
        "#####################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Build a simple network\n",
        "class simple_network(nn.Module):\n",
        "  def __init__(self, input_dim=1, num_class=10):\n",
        "    super(simple_network, self).__init__()\n",
        "\n",
        "    # Fully connected layers\n",
        "    self.fc1 = nn.Linear(784, num_class)\n",
        "    # self.fc1 = nn.Linear(784, 512)\n",
        "    #nn.init.kaiming_uniform_(self.fc1.weight)\n",
        "    # self.fc2 = nn.Linear(512, num_class)\n",
        "    #nn.init.kaiming_uniform_(self.fc2.weight)\n",
        "    # self.fc3 = nn.Linear(196, num_class)\n",
        "\n",
        "    # Convolutional layers\n",
        "    # self.conv1 = nn.Conv2d(input_dim, 20,  kernel_size=5, stride=1, padding=0)\n",
        "    # self.conv2 = nn.Conv2d(    20,    50,  kernel_size=5, stride=1, padding=0)\n",
        "    \n",
        "    # Activation func.\n",
        "    # self.relu = nn.ReLU()\n",
        "\n",
        "    # self.dropout1 = nn.Dropout(0.2)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # x = self.relu(self.fc1(x))                   # print(\"Trainig Data is limited, only the first \"+str(self.data.size(0))+\" samples are used.\")\n",
        "    # x = self.relu(self.fc2(x))     \n",
        "    x = F.relu(self.fc1(x))  \n",
        "    # x = self.dropout1(x)\n",
        "    # x = F.relu(self.fc2(x))   \n",
        "    # x = F.relu(self.conv1(x))\n",
        "    # x = F.relu(self.fc3(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 513,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "745uYVv6RtoN",
        "outputId": "a63cf309-a6c7-4442-c7f2-352c6e03e401"
      },
      "source": [
        "##########################################################################\n",
        "### Prepare the trainloader/testloader for training/validating network ###\n",
        "##########################################################################\n",
        "\n",
        "from   torchvision import datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils as utils\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# The 「transform」 is used to \n",
        "# i)  convert PIL.Image to torch.FloatTensor (batch, dim, H, W), and change the \n",
        "#     inputs' range to [0, 1] (by inputs/= 255.0);\n",
        "# ii) standardize the input images by mean=0.1307, std=0.3081\n",
        "\n",
        "# transform = transforms.Compose([transforms.ToTensor(),\n",
        "#                                 transforms.Normalize((0.1307,), (0.3081,))])\n",
        "# transform = transforms.Normalize((0.1307,), (0.3081,))\n",
        "transform = transforms.Compose([\n",
        "                                # transforms.RandomAffine(degrees=[-10.0, 10.0]),\n",
        "                                # transforms.RandomPerspective(),\n",
        "                                # transforms. RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                # transforms.RandomErasing(scale=(0.02, 0.33)),\n",
        "                                # transforms.GaussianBlur(kernel_size=1),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                                ])\n",
        "print(transform)\n",
        "\n",
        "# Initialize dataloaders.\n",
        "limit_data = 1000 # The first $limit_data samples\n",
        "mnist_train = MyMNIST('./data', train=True,  download=True, transform=transform, limit_data=limit_data)\n",
        "mnist_test  = MyMNIST('./data', train=False, download=True, transform=transform)\n",
        "trainloader = utils.data.DataLoader(mnist_train, batch_size=50, shuffle=True,  num_workers=2)\n",
        "testloader  = utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "execution_count": 514,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    ToTensor()\n",
            ")\n",
            "[WRN]: Trainig Data is limited, only the first 1000 samples will be used.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmizLKieRvv9",
        "outputId": "523a88bf-4f81-4921-8d8c-a0c306629e16"
      },
      "source": [
        "# Initialize the network\n",
        "net = simple_network().cuda()\n",
        "# net = LeNet().cuda()\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 515,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simple_network(\n",
            "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HloPsyuWRwVq"
      },
      "source": [
        "############################################\n",
        "### Prepare the optimizer and loss func. ###\n",
        "############################################\n",
        "\n",
        "import torch.optim as optim\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0)\n",
        "# optimizer = optim.Adam(net.parameters())\n",
        "epoch = 0"
      ],
      "execution_count": 516,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opAM29jLR2VI"
      },
      "source": [
        "######################################################\n",
        "#### ------ Script for evaluate the network ------ ###\n",
        "######################################################\n",
        "\n",
        "def evaluate_model():\n",
        "  print(\"Testing the network...\")\n",
        "  net.eval()\n",
        "  total_num   = 0\n",
        "  correct_num = 0\n",
        "  for test_iter, test_data in enumerate(testloader):\n",
        "    # Get one batch of test samples\n",
        "    inputs, labels = test_data    \n",
        "    bch = inputs.size(0)\n",
        "    inputs = inputs.view(bch, -1)\n",
        "\n",
        "    # Move inputs and labels into GPU\n",
        "    inputs = inputs.cuda()\n",
        "    labels = torch.LongTensor(list(labels)).cuda()\n",
        "\n",
        "    # Forward\n",
        "    outputs = net(inputs)   \n",
        "\n",
        "    # Get predicted classes\n",
        "    _, pred_cls = torch.max(outputs, 1)\n",
        "    # if total_num == 0:\n",
        "    #    print(\"True label:\\n\", labels)\n",
        "    #    print(\"Prediction:\\n\", pred_cls)\n",
        "    # # Record test result\n",
        "    correct_num+= (pred_cls == labels).float().sum().item()\n",
        "    total_num+= bch\n",
        "  net.train()\n",
        "  \n",
        "  print(\"Accuracy: \"+\"%.2f\"%(correct_num/float(total_num)))"
      ],
      "execution_count": 517,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6PMBVJmR4tL",
        "outputId": "23b2d93f-4770-495e-ca58-0dd0649fd13f"
      },
      "source": [
        "#####################################################\n",
        "### ------ Script for training the network ------ ###\n",
        "#####################################################\n",
        "\n",
        "epoch_size = 10\n",
        "\n",
        "# add\n",
        "total_num   = 0\n",
        "correct_num = 0\n",
        "# add-end\n",
        "\n",
        "for epoch_idx in range(epoch_size):\n",
        "  running_loss = 0.0\n",
        "  ct_num       = 0\n",
        "  for iteration, data in enumerate(trainloader):\n",
        "    # Take the inputs and the labels for 1 batch.\n",
        "    inputs, labels = data\n",
        "    bch = inputs.size(0)\n",
        "    inputs = inputs.view(bch, -1)\n",
        "    \n",
        "    # Move inputs and labels into GPU\n",
        "    inputs = inputs.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    # Remove old gradients for the optimizer.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute result (Forward)\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Compute loss\n",
        "    loss    = loss_func(outputs, labels)\n",
        "\n",
        "    # Calculate gradients (Backward)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    #with torch.no_grad():\n",
        "    running_loss += loss.item()\n",
        "    ct_num+= 1\n",
        "    if iteration%20 == 19:\n",
        "      # add\n",
        "      # _, pred_cls = torch.max(outputs, 1)\n",
        "      # correct_num+= (pred_cls == labels).float().sum().item()\n",
        "      # total_num+= bch\n",
        "      # add-end\n",
        "      print(\"[Epoch: \"+str(epoch+1)+\"]\"\" --- Iteration: \"+str(iteration+1)+\", Loss: \"+str(running_loss/ct_num)+'.')\n",
        "\n",
        "    # Test\n",
        "    if epoch%3 == 2 and iteration%100 == 99:\n",
        "      evaluate_model()\n",
        "    \n",
        "  epoch += 1"
      ],
      "execution_count": 518,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1] --- Iteration: 20, Loss: 2.2825344800949097.\n",
            "[Epoch: 2] --- Iteration: 20, Loss: 2.1922790884971617.\n",
            "[Epoch: 3] --- Iteration: 20, Loss: 2.086781072616577.\n",
            "[Epoch: 4] --- Iteration: 20, Loss: 1.992095011472702.\n",
            "[Epoch: 5] --- Iteration: 20, Loss: 1.9028127431869506.\n",
            "[Epoch: 6] --- Iteration: 20, Loss: 1.8185592234134673.\n",
            "[Epoch: 7] --- Iteration: 20, Loss: 1.7372877597808838.\n",
            "[Epoch: 8] --- Iteration: 20, Loss: 1.6702316761016847.\n",
            "[Epoch: 9] --- Iteration: 20, Loss: 1.6013930797576905.\n",
            "[Epoch: 10] --- Iteration: 20, Loss: 1.5402338624000549.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE80IKc1R69l",
        "outputId": "c2460dd1-e092-475b-dff1-0f11e9f0d8db"
      },
      "source": [
        "# print(\"Train Accuracy: \"+\"%.2f\"%(correct_num/float(total_num)))\n",
        "evaluate_model()"
      ],
      "execution_count": 519,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing the network...\n",
            "Accuracy: 0.64\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}